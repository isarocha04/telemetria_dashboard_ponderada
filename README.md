#  Dashboard de Telemetria do Data Lake e Data Warehouse

##  Vis√£o Geral
Este projeto consiste na cria√ß√£o de um **Dashboard de Telemetria** para monitoramento de um **Data Lake** e um **Data Warehouse** utilizando **Streamlit** e **Supabase**. O objetivo √© fornecer insights sobre o uso dos dados e a performance do sistema, garantindo que as informa√ß√µes sejam apresentadas de forma clara e eficaz.

##  Objetivos do Dashboard
- **Monitorar logs** do Data Warehouse para identificar padr√µes de uso e falhas.
- **Analisar os dados brutos** armazenados no Data Lake.
- **Exibir m√©tricas relevantes**, como volume de registros, taxa de erro, distribui√ß√£o por servi√ßo e n√≠vel de severidade.
- **Permitir filtros temporais** para refinar a an√°lise dentro de um intervalo de tempo selecionado pelo usu√°rio.

##  Tecnologias Utilizadas
- **Streamlit**: Framework para cria√ß√£o do dashboard.
- **Supabase**: Plataforma de banco de dados (PostgreSQL) utilizada para armazenar os dados.
- **Plotly**: Biblioteca para visualiza√ß√£o de dados.
- **Pandas**: Manipula√ß√£o e an√°lise dos dados.
- **Python dotenv**: Gerenciamento de vari√°veis de ambiente.


```

##  Configura√ß√£o e Execu√ß√£o
### 1Ô∏è Configurar as Credenciais do Supabase
Crie um arquivo `.env` na raiz do projeto e adicione:
```
SUPABASE_URL="https://seu-supabase-url.supabase.co"
SUPABASE_API_KEY="sua-chave-api"
```

### 2 Instalar Depend√™ncias
Execute o comando abaixo para instalar as bibliotecas necess√°rias:
```
pip install -r requirements.txt
```

### 3Ô∏è Executar o Dashboard
```
streamlit run app.py
```

##  M√©tricas Monitoradas
O dashboard √© dividido em duas se√ß√µes: **Logs** e **Bronze (Data Lake)**.

###  An√°lise de Logs (Data Warehouse)
1. **Evolu√ß√£o de logs ao longo do tempo** (por minuto)
2. **Quantidade total de logs** no per√≠odo selecionado
3. **Distribui√ß√£o de logs por status** (SUCCESS, ERROR, etc.)
4. **Taxa de erro** (percentual de logs com status "ERROR")
5. **Logs por servi√ßo e status** (identifica√ß√£o de falhas por servi√ßo)
6. **Logs por n√≠vel de severidade (level_type)**
7. **Distribui√ß√£o de logs por servi√ßo**

###  An√°lise de Dados Brutos (Data Lake)
1. **Evolu√ß√£o do volume de registros** ao longo do tempo (por minuto)
2. **Distribui√ß√£o dos registros por status (JSON)**
3. **Distribui√ß√£o dos registros por localiza√ß√£o (JSON)**
4. **An√°lise do campo "CAPTURE_TIME"** para verificar padr√µes

 **Padr√µes identificados:**
- A maioria dos logs foi gerada em um curto per√≠odo de tempo, o que indica que os dados foram inseridos em lote.
- Os registros do Data Lake apresentam diferentes status e localiza√ß√µes, permitindo identificar padr√µes de captura dos dados.

 **Pontos de aten√ß√£o:**
- A alta concentra√ß√£o de erros em determinados servi√ßos pode indicar falhas sist√™micas.
- A distribui√ß√£o dos logs por n√≠vel de severidade pode ser usada para priorizar a√ß√µes corretivas.
- A visualiza√ß√£o do volume de dados no Data Lake ajuda a prever necessidade de armazenamento.


## üì¢ Conclus√£o
Este dashboard fornece uma vis√£o detalhada da telemetria do sistema, auxiliando na identifica√ß√£o de falhas e melhorias na governan√ßa dos dados. A implementa√ß√£o permite monitoramento cont√≠nuo e suporte √† tomada de decis√µes baseadas em dados.

